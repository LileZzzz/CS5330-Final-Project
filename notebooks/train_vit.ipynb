{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e11722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torchvision.models import vit_b_16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6828e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=32):\n",
    "    # Define the transformation (Imagnet mean and std)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = datasets.EuroSAT(root='./data', download=True, transform=transform)\n",
    "\n",
    "    # Split the dataset into train, validation, test sets\n",
    "    train_size = int(0.01 * len(dataset))\n",
    "    val_size = int(0.01 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    \n",
    "    return dataset, train_dataloader, val_dataloader, test_dataloader   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained vit model\n",
    "def load_vit(num_classes=10, unfreeze=5):\n",
    "    model = vit_b_16(weights='DEFAULT')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze the last few layers\n",
    "    if unfreeze > 0:\n",
    "        encoder_layers = model.encoder.layers\n",
    "        number_of_layers = len(encoder_layers)\n",
    "        \n",
    "        for i in range(number_of_layers - unfreeze, number_of_layers):\n",
    "            for param in encoder_layers[i].parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    # replace the classifier head\n",
    "    num_features = model.heads.head.in_features\n",
    "    model.heads.head = torch.nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffcff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total_size += labels.size(0)\n",
    "        \n",
    "    return total_loss / total_size, 100 * correct / total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def eval(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            total_size += labels.size(0)\n",
    "        \n",
    "    return total_loss / total_size, 100 * correct / total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the model\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 5e-5, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    unfreeze = trial.suggest_categorical('unfreeze', [0, 6, 12])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 0.01, log=True)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load the data\n",
    "    dataset, train_dataloader, val_dataloader, test_dataloader = load_data(batch_size)\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_vit(num_classes=len(dataset.classes), unfreeze=unfreeze).to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\"):\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"unfreeze\", unfreeze)\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        \n",
    "        patience = 2\n",
    "        patience_cnt = 0\n",
    "        best_val_acc = 0\n",
    "        best_epoch = 0\n",
    "        epochs = 5\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
    "            val_loss, val_acc = eval(model, val_dataloader, criterion, device)\n",
    "            \n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "            \n",
    "            trial.report(val_acc, epoch)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                \n",
    "                if patience_cnt >= patience:\n",
    "                    break\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "    return best_val_acc, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the predictions by running the model on the test set\n",
    "def visualize_predictions(model, dataloader, device, dataset, n_samples=25):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # visualize the first n_samples images\n",
    "            fig, axes = plt.subplots(5, 5, figsize=(30, 20))\n",
    "            for i in range(n_samples):\n",
    "                ax = axes[i // 5, i % 5]\n",
    "                # unnormalize the image\n",
    "                image = images[i].cpu().permute(1, 2, 0)\n",
    "                image = image * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "                image = image.clamp(0, 1)\n",
    "                ax.imshow(image)\n",
    "                # set the title to the predicted and true label not number\n",
    "                ax.set_title(f\"Pred: {dataset.classes[preds[i].item()]}, True: {dataset.classes[labels[i].item()]}\")\n",
    "                ax.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            break\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pyngrok import ngrok, conf\n",
    "import getpass\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"\n",
    "EXPERIMENT_NAME = \"EuroSAT_ViT_Classification\"\n",
    "\n",
    "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI, \"--port\", \"8080\"])\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name=EXPERIMENT_NAME)\n",
    "study.optimize(objective, n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ed97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f365e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_dataloader, val_dataloader, test_dataloader = load_data()\n",
    "print(len(train_dataloader.dataset), len(val_dataloader.dataset), len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c289a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vit()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of parameters in the model in terms of millions\n",
    "num_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"Number of parameters in the model: {num_params:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aafbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = eval(model, val_dataloader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "#test_loss, test_acc = eval(model, test_dataloader, criterion, device)\n",
    "#print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, test_dataloader, device, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
